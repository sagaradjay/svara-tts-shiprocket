[supervisord]
nodaemon=true
logfile=/var/log/supervisor/supervisord.log
pidfile=/var/run/supervisord.pid
childlogdir=/var/log/supervisor
loglevel=info
user=root

[program:vllm]
environment=VLLM_TENSOR_PARALLEL_SIZE=%(ENV_VLLM_TENSOR_PARALLEL_SIZE)s
command=python3 -m vllm.entrypoints.openai.api_server --model %(ENV_VLLM_MODEL)s --host %(ENV_VLLM_HOST)s --port %(ENV_VLLM_PORT)s --gpu-memory-utilization %(ENV_VLLM_GPU_MEMORY_UTILIZATION)s --max-model-len %(ENV_VLLM_MAX_MODEL_LEN)s --tensor-parallel-size %(ENV_VLLM_TENSOR_PARALLEL_SIZE)s --trust-remote-code --dtype auto --hf-overrides '{"model_max_length": %(ENV_VLLM_MAX_MODEL_LEN)s}'
directory=/app
autostart=true
autorestart=true
startretries=3
redirect_stderr=true
stdout_logfile=/var/log/supervisor/vllm.log
stdout_logfile_maxbytes=50MB
stdout_logfile_backups=10
stderr_logfile=/var/log/supervisor/vllm_error.log
stderr_logfile_maxbytes=50MB
stderr_logfile_backups=10
priority=100
stopwaitsecs=30
killasgroup=true
stopasgroup=true

[program:fastapi]
command=python3 -m uvicorn server:app --host %(ENV_API_HOST)s --port %(ENV_API_PORT)s --log-level info --no-access-log
directory=/app/api
autostart=true
autorestart=true
startretries=3
redirect_stderr=true
stdout_logfile=/var/log/supervisor/fastapi.log
stdout_logfile_maxbytes=50MB
stdout_logfile_backups=10
stderr_logfile=/var/log/supervisor/fastapi_error.log
stderr_logfile_maxbytes=50MB
stderr_logfile_backups=10
priority=200
startsecs=10
stopwaitsecs=10
killasgroup=true
stopasgroup=true
# Wait for vLLM to be ready before starting
depends_on=vllm
